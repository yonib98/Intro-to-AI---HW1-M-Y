{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AHjIAUOGMLiI"
   },
   "source": [
    "# Welcome to Intro to Artificial Inteligence assignment 1 practical part!\n",
    "\n",
    "In this part you will get to implement algotithms learned in class from scratch.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pupose of this notebook is to provide the requirments of the practical part and some visualizations. Your implementation should be in the provided Algorithms.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUhq1ubdDE2v"
   },
   "source": [
    "**Important tip:** If the same variable name appears in two different cells, the variable value will be determined by the last cell to run, rather than by the position of the cell. Let's see an example,  run the three cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W36S7jvkENyg"
   },
   "outputs": [],
   "source": [
    "# cell 1\n",
    "x = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kei_W6-QEQD_"
   },
   "outputs": [],
   "source": [
    "# cell 2\n",
    "x = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JPQkx5XQETF5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO3WdjItEVQh"
   },
   "source": [
    "now rerun cell 1 and print x again.\n",
    "\n",
    "The same applies to functions, classes, etc.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ik_YMt0HO_R9"
   },
   "source": [
    "# The Environment\n",
    "\n",
    "\n",
    "1. Your goal is to find a route from the initial state to **a** goal state.\n",
    "2. Each position on the map is marked with a letter, and each letter has a different meaning:\n",
    "\n",
    "  *   S - Initial (start) position. There's only one initial state on the map and its always on the upper-left corner of the grid.\n",
    "  *   G - Final state. As learned in class there may be multiple goal states.\n",
    "  *   H - Hole. When your agent reaches a hole, it falls into the water and cannot continue walking.\n",
    "  *   D - Dragon Ball. There are either two on the map. When the agent reaches a Dragon Ball, it is immediately collected. You can assume that they will not appear in a final or initial state.\n",
    "  *   F - This is the most common square on the map. The agent can walk on it safely.\n",
    "\n",
    "  Your agent can move faster by collecting 3 special objects.\n",
    "\n",
    "  *   T - [Talaria](https://en.wikipedia.org/wiki/Talaria). A pair of Winged Sandals that help you fly to the square.\n",
    "  *   A - Air jorden. Jumping shoes that help you jump to the square.\n",
    "  *   L - Lightning. Makes you run faster to the square.\n",
    "\n",
    "  When the agent performs transition (s,a,s') and collects one of the objects (T,A,L) at state s', the collected object changes the cost of the transition (as detailed below). \n",
    "\n",
    "3. The cost of each transition (s,a,s') is based on the mark if the square s' you pass to:\n",
    "  * S - 1\n",
    "  * G - 1\n",
    "  * H - Ö¿$\\infty$\n",
    "  * D - 1\n",
    "  * F - 10\n",
    "  * T - 3\n",
    "  * A - 2\n",
    "  * L - 1\n",
    "\n",
    "4. The pink square marks where your agent is.\n",
    "\n",
    "5. The agent position is calculated as follows: row_number *num_col + col_ number*.\n",
    "\n",
    "6. Our agent can perform 4 actions:\n",
    "  * 0 - Down\n",
    "  * 1 - Right\n",
    "  * 2 - Up\n",
    "  * 3 - Left\n",
    "  \n",
    "\n",
    "7. If the agent tries to move outside the board boundaries, he stays in the same place.\n",
    "\n",
    "8. Section 6 describes the order in which the nodes should be created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XIvTBylGni-J"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "from DragonBallEnv import DragonBallEnv\n",
    "from typing import List, Tuple\n",
    "from Algorithms import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iHTZ1L7bcMlw"
   },
   "outputs": [],
   "source": [
    "DOWN = 0\n",
    "RIGHT = 1\n",
    "UP = 2\n",
    "LEFT = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EWEiS5mVn7mg"
   },
   "source": [
    "# Maps\n",
    "A map can be produced manually as shown in the cell below. We will only work on maps in which there is a route from the initial state to a final state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MaIE0Yqenp2O"
   },
   "outputs": [],
   "source": [
    "MAPS = {\n",
    "    \"4x4\": [\"SFFF\",\n",
    "            \"FDFF\",\n",
    "            \"FFFD\",\n",
    "            \"FFFG\"],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFTAL\",\n",
    "        \"TFFHFFTF\",\n",
    "        \"FFFFFHTF\",\n",
    "        \"FAFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"DFTFHDTL\",\n",
    "        \"FLFHFFFG\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIpo4KAgxE9q"
   },
   "source": [
    "\n",
    "# The Dragon Ball Environment â›·\n",
    "The file `DragonBallEnv.py` implements our own version of the dragon ball environment. It is recommended to go through the code. \n",
    "\n",
    "**Note: You are not allowed to change this file.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRBezhLKIIJH"
   },
   "source": [
    "Lets start by creating a new environment object.\n",
    "\n",
    "In order to create an environment object, you must provide it with a board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9ce62lyU8L39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: (0, False, False)\n",
      "Goal states: [(63, True, True)]\n"
     ]
    }
   ],
   "source": [
    "env = DragonBallEnv(MAPS[\"8x8\"])\n",
    "state = env.reset()\n",
    "print('Initial state:', state)\n",
    "print('Goal states:', env.goals) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWYU-zl1J1_m"
   },
   "source": [
    "First, take a look at the state space $\\mathcal{S}$ (all possible states) and action space $\\mathcal{A}$ (all possible actions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Cg7XAAByJ-IH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(4)\n",
      "State Space Tuple(Discrete(64), Discrete(2), Discrete(2))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action Space {env.action_space}\")\n",
    "print(f\"State Space {env.observation_space}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wqptD9FjiYni"
   },
   "source": [
    "***Remark***: You may have noticed that gym uses `observation_space` instead of state space. For the purpose of this homework, the state space is the same as the observation space. However, in some problems the state is partially observed, so the space of possible states may not be the same as the space of possible observations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BASrtg6SHjFI"
   },
   "source": [
    "Now we will go through some usfel methods (It is still recommended to go through the other methods in the class):\n",
    "\n",
    "`render()` - returns a printable view of the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kDNpfddaBWJk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[43m\u001b[45mS\u001b[0m\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mT\u001b[0m\u001b[42mA\u001b[0m\u001b[42mL\u001b[0m\n",
      "\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mA\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[43mD\u001b[0m\u001b[42mF\u001b[0m\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[43mD\u001b[0m\u001b[42mT\u001b[0m\u001b[42mL\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mL\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[43mG\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(env.render())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAaEmhts9vAT"
   },
   "source": [
    "The pink square represents the agent. The letter ×´*H*×´ represents holes, and the yellow square are dragon balls or goal.\n",
    "\n",
    "Here are two more useful methods:\n",
    "\n",
    "`get_state()` - Returns the current state of the agent.\n",
    "\n",
    "`set_state(state)` - Sets the current state of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "43Z9On8M--ln"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[43mS\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mT\u001b[0m\u001b[42mA\u001b[0m\u001b[42mL\u001b[0m\n",
      "\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\u001b[45m\u001b[45mF\u001b[0m\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mA\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[43mD\u001b[0m\u001b[42mF\u001b[0m\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[43mD\u001b[0m\u001b[42mT\u001b[0m\u001b[42mL\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mL\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[43mG\u001b[0m\n",
      "\n",
      "the agent is at state: (18, False, False)\n"
     ]
    }
   ],
   "source": [
    "env.set_state((18, False, False))\n",
    "print(env.render())\n",
    "print(f\"the agent is at state: {env.get_state()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VwmIbSyAcsC"
   },
   "source": [
    "`succ(state)` - Returns a dictionary that contains information on all the successors of a state.\n",
    "\n",
    "*   The keys are the actions.\n",
    "*   The values are tuples of the form (next state, cost, terminated). Note that terminated is true when the agent reaches a **final state** or a **hole**.\n",
    "\n",
    "\n",
    "\n",
    "***Tip***: You can loop through both keys and values by using the `items()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: (18, False, False)\n",
      "\n",
      "*** Action: 0 ***\n",
      "Next state: (26, False, False)\n",
      "Cost: 10.0\n",
      "Terminated: False\n",
      "\n",
      "*** Action: 1 ***\n",
      "Next state: (19, False, False)\n",
      "Cost: inf\n",
      "Terminated: True\n",
      "\n",
      "*** Action: 2 ***\n",
      "Next state: (10, False, False)\n",
      "Cost: 10.0\n",
      "Terminated: False\n",
      "\n",
      "*** Action: 3 ***\n",
      "Next state: (17, False, False)\n",
      "Cost: 10.0\n",
      "Terminated: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_state = env.get_state()\n",
    "print(f\"Current state: {current_state}\\n\")\n",
    "for action, successor in env.succ(current_state).items():\n",
    "  print(f\"*** Action: {action} ***\")\n",
    "  print(f\"Next state: {successor[0]}\")\n",
    "  print(f\"Cost: {successor[1]}\")\n",
    "  print(f\"Terminated: {successor[2]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2qFBBDWBcVi"
   },
   "source": [
    "As you can see, the action 0 (down) will move your agent to state 26 and the transition will cost you 10. Action 1 will move your agent to state 19, which is a hole that will terminate your run.\n",
    "\n",
    "`is_final_state(state)` can assist you in distinguishing between a final state and a hole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7j097OzTBZ_4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next state: (19, False, False)\n",
      "Cost: inf\n",
      "Terminated: True\n",
      "Final state: False\n"
     ]
    }
   ],
   "source": [
    "state, cost, terminated = env.succ(current_state)[1]\n",
    "\n",
    "print(f\"Next state: {state}\")\n",
    "print(f\"Cost: {cost}\")\n",
    "print(f\"Terminated: {terminated}\")\n",
    "print(f\"Final state: {env.is_final_state(state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptr9kbNole-Y"
   },
   "source": [
    "Let's see what happens when we apply succ(state) on a hole:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GcpKGM5yli4M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: 19\n",
      "\n",
      "*** Action: 0 ***\n",
      "Next state: None\n",
      "Cost: None\n",
      "Terminated: None\n",
      "\n",
      "*** Action: 1 ***\n",
      "Next state: None\n",
      "Cost: None\n",
      "Terminated: None\n",
      "\n",
      "*** Action: 2 ***\n",
      "Next state: None\n",
      "Cost: None\n",
      "Terminated: None\n",
      "\n",
      "*** Action: 3 ***\n",
      "Next state: None\n",
      "Cost: None\n",
      "Terminated: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current state: 19\\n\")\n",
    "for action, (state, cost, terminated) in env.succ((19, False, False)).items():\n",
    "  print(f\"*** Action: {action} ***\")\n",
    "  print(f\"Next state: {state}\")\n",
    "  print(f\"Cost: {cost}\")\n",
    "  print(f\"Terminated: {terminated}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxny4BVOr-UT"
   },
   "source": [
    "As you can see, if the operator cannot be applied to the state, all returned values are \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXDKjWAoCqW_"
   },
   "source": [
    "Now it's time to move your agent around ðŸ¤–.\n",
    "\n",
    "`step(action)` - will move your agent one step along the board.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gLWj_oYaFM33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "\u001b[43mS\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mT\u001b[0m\u001b[42mA\u001b[0m\u001b[42mL\u001b[0m\n",
      "\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[45m\u001b[45mF\u001b[0m\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mA\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[43mD\u001b[0m\u001b[42mF\u001b[0m\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[43mD\u001b[0m\u001b[42mT\u001b[0m\u001b[42mL\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mL\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[43mG\u001b[0m\n",
      "\n",
      "New state: (26, False, False)\n",
      "cost: 10.0\n",
      "Terminated: False\n"
     ]
    }
   ],
   "source": [
    "new_state, cost, terminated = env.step(DOWN)\n",
    "print(env.render())\n",
    "print(\"New state:\", new_state)\n",
    "print(\"cost:\", cost)\n",
    "print(\"Terminated:\", terminated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHcHkCw8FipY"
   },
   "source": [
    "The step-function returns the following information:\n",
    "* __New state__: The state after the action is taken.\n",
    "* __Cost__: The immediate cost.\n",
    "* __Terminated__: Is the environment done? In our environment this will be false until the agent will reach a hole or a final state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qh9NM7qDlxUc"
   },
   "source": [
    "Here are a few more useful attributes and methods:\n",
    "\n",
    "\n",
    "`env.nrow`, `env.ncol` - Row and columns number.\n",
    "\n",
    "`env.nA` - Number of actions.\n",
    "\n",
    "`env.nS` - Number of squares.\n",
    "\n",
    "`env.lastaction` - The last action performed by the agent.\n",
    "\n",
    "`env.d1`, `env.d2` - This is the state for each dragon ball. If there are no dragon balls on the board they are set to NULL.\n",
    "\n",
    "`env.inc(row, col, action)` - Given a position and an action, returns the new position.\n",
    "\n",
    "`env.to_row_col(state)` - Converts between state and location on the board.\n",
    "\n",
    "`env.to_state(row, col)` - Converts between location on the board and state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoxs3sG0QyhF"
   },
   "source": [
    "We've finished our demo ðŸ¥³ and it's time to reset the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MfjMsGRnHoK0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state befor reset: (26, False, False)\n",
      "current state after reset: (0, False, False)\n"
     ]
    }
   ],
   "source": [
    "print(f\"current state befor reset: {env.get_state()}\")\n",
    "env.reset()\n",
    "print(f\"current state after reset: {env.get_state()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYqbtBE_kzDK"
   },
   "source": [
    "One (quite bad) strategy for out agent is to take a random action every time. Inside a gym-environment this can be done using `env.action_space.sample()`, which samples a random action from the action space. Look through the following loop and make sure that you understand what's going on. Here, we use `clear_output()` to clear the output of the Jupyter cell, and `time.sleep()` to pause between each action)\n",
    "\n",
    "\n",
    "Let's see what would happen if we try to brute-force our way to solving the problem.\n",
    "\n",
    "\n",
    "We'll create an infinite loop that runs until the agent reaches the final state.The `env.action_space.sample()` method automatically selects one random action from set of all possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MZE__OgBx78N"
   },
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "  def __init__(self):\n",
    "    self.env = None\n",
    "\n",
    "  def animation(self, epochs: int ,state: int, action: List[int], total_cost: int) -> None:\n",
    "      clear_output(wait=True)\n",
    "      print(self.env.render())\n",
    "      print(f\"Timestep: {epochs}\")\n",
    "      print(f\"State: {state}\")\n",
    "      print(f\"Action: {action}\")\n",
    "      print(f\"Total Cost: {total_cost}\")\n",
    "      time.sleep(1)\n",
    "\n",
    "  def random_search(self, DragonBallEnv: env) -> Tuple[List[int],int]:\n",
    "    self.env = env\n",
    "    self.env.reset()\n",
    "    epochs = 0\n",
    "    cost = 0\n",
    "    total_cost = 0\n",
    "\n",
    "    actions = []\n",
    "\n",
    "    state = self.env.get_initial_state()\n",
    "    while not self.env.is_final_state(state):\n",
    "      action = self.env.action_space.sample()\n",
    "      new_state, cost, terminated = self.env.step(action)\n",
    "        \n",
    "      while terminated is True and self.env.is_final_state(state) is False:\n",
    "        self.env.set_state(state)\n",
    "        action = self.env.action_space.sample()\n",
    "        new_state, cost, terminated = self.env.step(action)\n",
    "        \n",
    "      actions.append(action)\n",
    "      total_cost += cost\n",
    "      state = new_state\n",
    "      epochs += 1\n",
    "      \n",
    "      self.animation(epochs,state,action,total_cost)\n",
    "\n",
    "    return (actions, total_cost)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QJ9FDJWNZr3-"
   },
   "source": [
    "Let's check out this agent's performance!\n",
    "\n",
    "The output of this agent is the sequence of actions that led to the solution and the route's cost. \n",
    "\n",
    "Our random agent is not very successful, so we'll print his actions as they happen. \n",
    "\n",
    "1.   **Stop his run in the middle if you are tired of looking at him.**\n",
    "2.   After watching the agent please put the code in the box below in the a comment for your comfort.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8Gc3-gJVZH3h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "\u001b[43mS\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mT\u001b[0m\u001b[42mA\u001b[0m\u001b[45m\u001b[45mL\u001b[0m\u001b[0m\n",
      "\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mA\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\n",
      "\u001b[43mD\u001b[0m\u001b[42mF\u001b[0m\u001b[42mT\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[43mD\u001b[0m\u001b[42mT\u001b[0m\u001b[42mL\u001b[0m\n",
      "\u001b[42mF\u001b[0m\u001b[42mL\u001b[0m\u001b[42mF\u001b[0m\u001b[47mH\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[42mF\u001b[0m\u001b[43mG\u001b[0m\n",
      "\n",
      "Timestep: 25\n",
      "State: (15, False, False)\n",
      "Action: 1\n",
      "Total Cost: 157.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m RandomAgent()\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 38\u001b[0m, in \u001b[0;36mRandomAgent.random_search\u001b[0;34m(self, DragonBallEnv)\u001b[0m\n\u001b[1;32m     35\u001b[0m   state \u001b[38;5;241m=\u001b[39m new_state\n\u001b[1;32m     36\u001b[0m   epochs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 38\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtotal_cost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (actions, total_cost)\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36mRandomAgent.animation\u001b[0;34m(self, epochs, state, action, total_cost)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Cost: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_cost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# agent = RandomAgent()\n",
    "# agent.random_search(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW7YKLjCi4Qf"
   },
   "source": [
    "**Did you remember to put the code above in a comment?!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qBKDB1Aja5JB"
   },
   "source": [
    "As you can see, a random policy is, unsurprisingly, not a good policy. However, what else can we do?\n",
    "\n",
    "This is where you come in!\n",
    "\n",
    "In this assignment you will be required to implement the following algorithms taught in class in order to solve the problem.\n",
    "\n",
    "Algorithms: \n",
    "1. BFS-G\n",
    "2. W-A*\n",
    "3. epsilon-A*\n",
    "\n",
    "Important to note!\n",
    "\n",
    "Each agent should return a tuple: (actions, cost, expended) \n",
    "*  actions - the list of integers containing the sequence of actions that produce your agent's solution (and not the entire search process).\n",
    "* cost -  an integer which holds the total cost of the solution.\n",
    "* expanded - an integer which holds the number of nodes that have been expanded during the search (A node is considered expanded if we check for it's successors).\n",
    "\n",
    "The solution to our search problem is the a to the final state, not the final state itself. By saving the actions, we are able to restore the path your agent found.\n",
    "\n",
    "\n",
    "Any other output, unless otherwise specified, will cause the running of the tests to fail and will result in a grade of 0 !\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-nikK0HKwhRw"
   },
   "source": [
    "\n",
    "Some Tips:\n",
    "1. Follow the pseudo-code shown in the lectures and tutorials.\n",
    "2. You should write all your code within the classes. This way, we prevent overlapping functions with the same name while running the notebook.\n",
    "3. You may implement your code as you like but consider inherenting from the a general \"Agent\" class and implement some utilty methods such as the \"solution\" method which recieves a node and returns a path (sequence of actions) leading to that node.\n",
    "4. Consider implementing a \"node\" class.\n",
    "5. Using small boards will help you debug.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMwTzaJKw9gA"
   },
   "source": [
    "The function below (`print_solution()`) can be used for debugging purposes. It prints the sequence of actions it receives. The function will not be used to test your code, so you are welcome to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQj77NFT0Wdc"
   },
   "outputs": [],
   "source": [
    "def print_solution(actions,env: DragonBallEnv) -> None:\n",
    "    env.reset()\n",
    "    total_cost = 0\n",
    "    print(env.render())\n",
    "    print(f\"Timestep: {1}\")\n",
    "    print(f\"State: {env.get_state()}\")\n",
    "    print(f\"Action: {None}\")\n",
    "    print(f\"Cost: {0}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for i, action in enumerate(actions):\n",
    "      state, cost, terminated = env.step(action)\n",
    "      total_cost += cost\n",
    "      clear_output(wait=True)\n",
    "\n",
    "      print(env.render())\n",
    "      print(f\"Timestep: {i + 2}\")\n",
    "      print(f\"State: {state}\")\n",
    "      print(f\"Action: {action}\")\n",
    "      print(f\"Cost: {cost}\")\n",
    "      print(f\"Total cost: {total_cost}\")\n",
    "      \n",
    "      time.sleep(1)\n",
    "\n",
    "      if terminated is True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zGQKO6ka-5P"
   },
   "source": [
    "## 1. BFS-G\n",
    "**TO DO:** implement Breadth First Search (BFS) algorithm on graph like shown in class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfoBu-elP2To"
   },
   "source": [
    "Now lets test your BFS agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BFS_agent = BFSAgent()\n",
    "actions, total_cost, expanded = BFS_agent.search(env)\n",
    "print(f\"Total_cost: {total_cost}\")\n",
    "print(f\"Expanded: {expanded}\")\n",
    "print(f\"Actions: {actions}\")\n",
    "\n",
    "assert total_cost == 119.0, \"Error in total cost returned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_solution(actions, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Heapdict\n",
    "For the next algorithms, you will be required to maintain an \"open\" queue based on a certain value (g/h/v). To manage these queues efficiently and conveniently, please use [Heapdict](https://www.geeksforgeeks.org/priority-queue-using-queue-and-heapdict-module-in-python/). Heapdict implements the MutableMapping ABC, meaning it works pretty much like a regular Python [dictionary](https://www.geeksforgeeks.org/python-dictionary/). Itâ€™s designed to be used as a priority queue. Along with functions provided by ordinary dict(), it also has popitem() and peekitem() functions which return the pair with the lowest priority.\n",
    "\n",
    "Note:\n",
    "\n",
    "When two nodes have the same minimum value, select the node with the lower state index first. Instead of defining priority as an integer, you can define it as a tuple (value, state, ...)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sUnh0cwqrzfK"
   },
   "source": [
    "## 2. Weighted A*\n",
    "TO DO: implement Wighted A* like shown in class.\n",
    "\n",
    "Note:\n",
    "\n",
    "*   A parameter called `h_weight` is passed to `Greedy_Best_First_search()`, which indicates how much weight is given to the heuristics (ranging from 0 to 1).\n",
    "*   The heurisitcs needed to be implemented. Instructions in dry pdf.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xKSoHrMvJTG"
   },
   "outputs": [],
   "source": [
    "WA_agent = WeightedAStarAgent()\n",
    "actions, total_cost, expanded = WA_agent.search(env, h_weight=0.5)\n",
    "print(f\"Total_cost: {total_cost}\")\n",
    "print(f\"Expanded: {expanded}\")\n",
    "print(f\"Actions: {actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Li9RGdElvetj"
   },
   "outputs": [],
   "source": [
    "print_solution(actions, env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A*-epsilon::\n",
    "TO DO: implement A*-epsilon: like shown in class.\n",
    "\n",
    "use the same heuristic as in previous sections.\n",
    "\n",
    "Note:\n",
    "*   A parameter called `epsilon` is passed to `A_star_epsilon_search()`.\n",
    "*   We will not test the amount of expanded nodes for this algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AStarEpsilon_agent = AStarEpsilonAgent()\n",
    "actions, total_cost, expanded = AStarEpsilon_agent.search(env, epsilon=100)\n",
    "print(f\"Total_cost: {total_cost}\")\n",
    "print(f\"Expanded: {expanded}\")\n",
    "print(f\"Actions: {actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_solution(actions, env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Benchmarking:\n",
    "In this section we want to compare the different search algorithms. The take-home message is that there is no \"one algorithm fits all\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "test_boards = {\n",
    "\"map12x12\": \n",
    "['SFAFTFFTHHHF',\n",
    "'AFLTFFFFTALF',\n",
    "'LHHLLHHLFTHD',\n",
    "'HALTHAHHADHF',\n",
    "'FFFTFHFFAHFL',\n",
    "'LLTHFFFAHFAT',\n",
    "'HAAFFALHTATF',\n",
    "'LLLFHFFHTLFH',\n",
    "'FATAFHTTFFAF',\n",
    "'HHFLHALLFTLF',\n",
    "'FFAFFTTAFAAL',\n",
    "'TAAFFFHAFHFG'],\n",
    "\"map15x15\": \n",
    "['SFTTFFHHHHLFATF',\n",
    "'ALHTLHFTLLFTHHF',\n",
    "'FTTFHHHAHHFAHTF',\n",
    "'LFHTFTALTAAFLLH',\n",
    "'FTFFAFLFFLFHTFF',\n",
    "'LTAFTHFLHTHHLLA',\n",
    "'TFFFAHHFFAHHHFF',\n",
    "'TTFFLFHAHFFTLFD',\n",
    "'TFHLHTFFHAAHFHF',\n",
    "'HHAATLHFFLFFHLH',\n",
    "'FLFHHAALLHLHHAT',\n",
    "'TLHFFLTHFTTFTTF',\n",
    "'AFLTDAFTLHFHFFF',\n",
    "'FFTFHFLTAFLHTLA',\n",
    "'HTFATLTFHLFHFAG'],\n",
    "\"map20x20\" : \n",
    "['SFFLHFHTALHLFATAHTHT',\n",
    "'HFTTLLAHFTAFAAHHTLFH',\n",
    "'HHTFFFHAFFFFAFFTHHHT',\n",
    "'TTAFHTFHTHHLAHHAALLF',\n",
    "'HLALHFFTHAHHAFFLFHTF',\n",
    "'AFTAFTFLFTTTFTLLTHDF',\n",
    "'LFHFFAAHFLHAHHFHFALA',\n",
    "'AFTFFLTFLFTAFFLTFAHH',\n",
    "'HTTLFTHLTFAFFLAFHFTF',\n",
    "'LLALFHFAHFAALHFTFHTF',\n",
    "'LFFFAAFLFFFFHFLFFAFH',\n",
    "'THHTTFAFLATFATFTHLLL',\n",
    "'HHHAFFFATLLALFAHTHLL',\n",
    "'HLFFFFHFFLAAFTFFDAFH',\n",
    "'HTLFTHFFLTHLHHLHFTFH',\n",
    "'AFTTLHLFFLHTFFAHLAFT',\n",
    "'HAATLHFFFHHHHAFFFHLH',\n",
    "'FHFLLLFHLFFLFTFFHAFL',\n",
    "'LHTFLTLTFATFAFAFHAAF',\n",
    "'FTFFFFFLFTHFTFLTLHFG']}\n",
    "\n",
    "test_envs = {}\n",
    "for board_name, board in test_boards.items():\n",
    "    test_envs[board_name] = DragonBallEnv(board)\n",
    "\n",
    "\n",
    "BFS_agent = BFSAgent()\n",
    "WAStar_agent = WeightedAStarAgent()\n",
    "\n",
    "weights = [0.5, 0.7, 0.9]\n",
    "\n",
    "agents_search_function = [\n",
    "    BFS_agent.search,\n",
    "]\n",
    "\n",
    "header = ['map',  \"BFS-G cost\",  \"BFS-G expanded\",\\\n",
    "           'WA* (0.5) cost', 'WA* (0.5) expanded', 'WA* (0.7) cost', 'WA* (0.7) expanded', 'WA* (0.9) cost', 'WA* (0.9) expanded']\n",
    "\n",
    "with open(\"results.csv\", 'w') as f:\n",
    "  writer = csv.writer(f)\n",
    "  writer.writerow(header)\n",
    "  for env_name, env in test_envs.items():\n",
    "    data = [env_name]\n",
    "    for agent in agents_search_function:\n",
    "      _, total_cost, expanded = agent(env)\n",
    "      data += [total_cost, expanded]\n",
    "    for w in weights:\n",
    "        _, total_cost, expanded = WAStar_agent.search(env, w)\n",
    "        data += [total_cost, expanded]\n",
    "\n",
    "    writer.writerow(data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
